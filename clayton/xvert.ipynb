{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, log_loss\n",
    "import xgboost as xgb\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Y:/departments/research_and_development/baseball_operations/clayton_goodiez/csv/2021_data.csv\")\n",
    "data_2022 = pd.read_csv(\"Y:/departments/research_and_development/baseball_operations/clayton_goodiez/csv/2022_data.csv\")\n",
    "data_2023 = pd.read_csv(\"Y:/departments/research_and_development/baseball_operations/clayton_goodiez/csv/2023_MLB_Seaspm.csv\")\n",
    "height_df = pd.read_csv(\"Y:/departments/research_and_development/baseball_operations/clayton_goodiez/csv/player_heights.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = pd.concat([data_2022, data_2023], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hit_into_play', 'ball', 'foul_bunt', 'blocked_ball',\n",
       "       'swinging_strike', 'foul', 'called_strike', 'foul_tip',\n",
       "       'hit_by_pitch', 'swinging_strike_blocked', 'missed_bunt',\n",
       "       'pitchout', 'bunt_foul_tip', 'unknown_strike'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data['description'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(df, height_df):\n",
    "    \"\"\"\n",
    "    This function takes a DataFrame and applies several preprocessing steps to it.\n",
    "    \"\"\"\n",
    "    # Define a function to condense pitch types\n",
    "    def condense_pitch_type(pitch_type):\n",
    "        if pitch_type == \"FF\":\n",
    "            return \"FF\"\n",
    "        elif pitch_type == \"FT\":\n",
    "            return \"SI\"\n",
    "        elif pitch_type == \"FC\":\n",
    "            return \"CT\"\n",
    "        elif pitch_type in [\"SL\", \"ST\", \"SV\"]:\n",
    "            return \"SL\"\n",
    "        elif pitch_type in [\"CH\", \"FS\"]:\n",
    "            return \"CH\"\n",
    "        elif pitch_type in [\"CU\", \"KC\", \"CS\"]:\n",
    "            return \"CB\"\n",
    "        elif pitch_type == \"SI\":\n",
    "            return \"SI\"\n",
    "        elif pitch_type == \"KN\":\n",
    "            return \"KN\"\n",
    "        else:\n",
    "            return \"FAHCK\"\n",
    "\n",
    "    def create_pitch_id(df):\n",
    "        df['pitch_id_raw'] = (df['game_pk'].astype(str) + \"_\" +\n",
    "                          df['batter'].astype(str) + \"_\" +\n",
    "                          df['pitcher'].astype(str) + \"_\" +\n",
    "                          df['pitch_number'].astype(str) + \"_\" +\n",
    "                          df['at_bat_number'].astype(str) + \"_\" +\n",
    "                          df['inning'].astype(str))\n",
    "        return df\n",
    "\n",
    "    df['pitch_type_condensed'] = df['pitch_type'].apply(condense_pitch_type)\n",
    "\n",
    "    # Calculate 'release_pos_y'\n",
    "    df['release_pos_y'] = 60.5 - df['release_extension']\n",
    "\n",
    "\n",
    "    # Create 'pitch_id_raw'\n",
    "    df = create_pitch_id(df)\n",
    "\n",
    "    dfs = df.merge(height_df[['id', 'height_numeric']], how='left', left_on='pitcher', right_on='id')\n",
    "    # Return the preprocessed DataFrame\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the 'pitch_type' column\n",
    "training_data = preprocess_data(data, height_df)\n",
    "testing_data_processed = preprocess_data(testing_data, height_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastball_training = training_data.query('pitch_type_condensed in (\"FF\", \"SI\", \"CT\")')\n",
    "fastball_testing = testing_data_processed.query('pitch_type_condensed in (\"FF\", \"SI\", \"CT\")')\n",
    "breaking_ball = training_data.query('pitch_type_condensed in (\"SL\", \"CB\")')\n",
    "breaking_ball_testing = testing_data_processed.query('pitch_type_condensed in (\"SL\", \"CB\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fastball Model First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, separate features and identifiers\n",
    "training_identifiers = fastball_training[['pitch_id_raw']]\n",
    "testing_identifiers = fastball_testing[['pitch_id_raw']]\n",
    "training_features = fastball_training.drop(columns=[\n",
    "    'description', 'events', 'game_pk', 'game_date', 'des', 'zone', 'stand', 'p_throws',\n",
    "    'spin_rate_deprecated', 'break_angle_deprecated', 'break_length_deprecated', \n",
    "    'game_type', 'home_team', 'away_team', 'type', 'bb_type', 'inning_topbot', \n",
    "    'hc_x', 'hc_y', 'tfs_deprecated', 'tfs_zulu_deprecated', 'hit_distance_sc', \n",
    "    'launch_speed', 'launch_angle', 'estimated_ba_using_speedangle',\n",
    "    'estimated_woba_using_speedangle', 'woba_value', 'woba_denom', 'babip_value', \n",
    "    'iso_value', 'launch_speed_angle', 'pitch_name', 'home_score', 'away_score', \n",
    "    'bat_score', 'fld_score', 'post_away_score', 'post_home_score', 'post_bat_score', \n",
    "    'post_fld_score', 'if_fielding_alignment', 'of_fielding_alignment', \n",
    "    'delta_home_win_exp', 'delta_run_exp', 'player_name', 'hit_location', 'umpire', \n",
    "    'sv_id', 'spin_dir', 'pitch_type', 'fielder_2.1', 'pitcher.1', 'fielder_3', \n",
    "    'fielder_4', 'fielder_5', 'fielder_6', 'fielder_7', 'fielder_8', 'fielder_9', \n",
    "    'batter', 'pitcher', 'on_3b', 'on_2b', 'on_1b', 'game_year', 'outs_when_up', \n",
    "    'pitch_number', 'at_bat_number', 'inning', 'fielder_2', 'sz_top', 'sz_bot',\n",
    "    'pitch_id_raw', 'effective_speed', 'p_throws', 'id', 'pitch_type_condensed',\n",
    "    'plate_x', 'plate_z'\n",
    "], axis=1)\n",
    "testing_features = fastball_testing.drop(columns=[\n",
    "    'description', 'events', 'game_pk', 'game_date', 'des', 'zone', 'stand', 'p_throws',\n",
    "    'spin_rate_deprecated', 'break_angle_deprecated', 'break_length_deprecated', \n",
    "    'game_type', 'home_team', 'away_team', 'type', 'bb_type', 'inning_topbot', \n",
    "    'hc_x', 'hc_y', 'tfs_deprecated', 'tfs_zulu_deprecated', 'hit_distance_sc', \n",
    "    'launch_speed', 'launch_angle', 'estimated_ba_using_speedangle',\n",
    "    'estimated_woba_using_speedangle', 'woba_value', 'woba_denom', 'babip_value', \n",
    "    'iso_value', 'launch_speed_angle', 'pitch_name', 'home_score', 'away_score', \n",
    "    'bat_score', 'fld_score', 'post_away_score', 'post_home_score', 'post_bat_score', \n",
    "    'post_fld_score', 'if_fielding_alignment', 'of_fielding_alignment', \n",
    "    'delta_home_win_exp', 'delta_run_exp', 'player_name', 'hit_location', 'umpire', \n",
    "    'sv_id', 'spin_dir', 'pitch_type', 'fielder_2.1', 'pitcher.1', 'fielder_3', \n",
    "    'fielder_4', 'fielder_5', 'fielder_6', 'fielder_7', 'fielder_8', 'fielder_9', \n",
    "    'batter', 'pitcher', 'on_3b', 'on_2b', 'on_1b', 'game_year', 'outs_when_up', \n",
    "    'pitch_number', 'at_bat_number', 'inning', 'fielder_2', 'sz_top', 'sz_bot',\n",
    "    'pitch_id_raw', 'effective_speed', 'p_throws', 'id', 'pitch_type_condensed',\n",
    "    'plate_x', 'plate_z', 'Unnamed: 0'\n",
    "], axis=1)   # Exclude the identifier from the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = training_features.drop(['pfx_z'], axis=1).copy()\n",
    "train_label = training_features['pfx_z'].copy()\n",
    "testing_data = testing_features.drop(['pfx_z'], axis=1).copy()\n",
    "testing_label = testing_features['pfx_z'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['release_speed', 'release_pos_x', 'release_pos_z', 'balls', 'strikes',\n",
       "       'pfx_x', 'vx0', 'vy0', 'vz0', 'ax', 'ay', 'az', 'release_spin_rate',\n",
       "       'release_extension', 'release_pos_y', 'spin_axis', 'height_numeric'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['release_speed', 'release_pos_x', 'release_pos_z', 'balls', 'strikes',\n",
       "       'pfx_x', 'vx0', 'vy0', 'vz0', 'ax', 'ay', 'az', 'release_spin_rate',\n",
       "       'release_extension', 'release_pos_y', 'spin_axis', 'height_numeric'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split: Separate out a portion of the data as a temporary test set\n",
    "x_temp, x_test, y_temp, y_test = train_test_split(\n",
    "    train_data, \n",
    "    train_label, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Second split: Split the temporary test set into validation and test sets\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_temp, \n",
    "    y_temp, \n",
    "    test_size=0.25,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[18:20:20] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07f6e447eee219473-1\\xgboost\\xgboost-ci-windows\\src\\objective\\regression_obj.cu:144: label must be in [0,1] for logistic regression",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\clayton.thompson\\OneDrive - Driveline Baseball\\Documents\\GitHub\\stuff-baby\\clayton\\xvert.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/clayton.thompson/OneDrive%20-%20Driveline%20Baseball/Documents/GitHub/stuff-baby/clayton/xvert.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/clayton.thompson/OneDrive%20-%20Driveline%20Baseball/Documents/GitHub/stuff-baby/clayton/xvert.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m6\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/clayton.thompson/OneDrive%20-%20Driveline%20Baseball/Documents/GitHub/stuff-baby/clayton/xvert.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39meta\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/clayton.thompson/OneDrive%20-%20Driveline%20Baseball/Documents/GitHub/stuff-baby/clayton/xvert.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mcolsample_bytree\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/clayton.thompson/OneDrive%20-%20Driveline%20Baseball/Documents/GitHub/stuff-baby/clayton/xvert.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/clayton.thompson/OneDrive%20-%20Driveline%20Baseball/Documents/GitHub/stuff-baby/clayton/xvert.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Initial Model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/clayton.thompson/OneDrive%20-%20Driveline%20Baseball/Documents/GitHub/stuff-baby/clayton/xvert.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39;49mtrain(params, dtrain)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[0;32m    182\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\core.py:2049\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2046\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2048\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 2049\u001b[0m     _check_call(\n\u001b[0;32m   2050\u001b[0m         _LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\n\u001b[0;32m   2051\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, ctypes\u001b[39m.\u001b[39;49mc_int(iteration), dtrain\u001b[39m.\u001b[39;49mhandle\n\u001b[0;32m   2052\u001b[0m         )\n\u001b[0;32m   2053\u001b[0m     )\n\u001b[0;32m   2054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2055\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\core.py:281\u001b[0m, in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \n\u001b[0;32m    272\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 281\u001b[0m     \u001b[39mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[39m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [18:20:20] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07f6e447eee219473-1\\xgboost\\xgboost-ci-windows\\src\\objective\\regression_obj.cu:144: label must be in [0,1] for logistic regression"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(x_test, label=y_test)\n",
    "params = {\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.1,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'logloss',\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 1,\n",
    "}\n",
    "\n",
    "# Initial Model\n",
    "model = xgb.train(params, dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred = model.predict(dtest)\n",
    "y_pred_binary = [1 if p >= 0.5 else 0 for p in y_pred]\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate Log Loss\n",
    "logloss = log_loss(y_test, y_pred)\n",
    "print(f\"Log Loss: {logloss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.title('Actual vs. Predicted Values')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red') # Line for perfect predictions\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Paramater Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(x_valid, label=y_valid)\n",
    "    \n",
    "    param = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3),\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1),\n",
    "        'scale_pos_weight': class_ratio\n",
    "    }\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    model = xgb.train(\n",
    "        param, \n",
    "        dtrain, \n",
    "        num_boost_round=100,  # Maximum number of boosting rounds\n",
    "        evals=[(dvalid, 'eval')],  # Validation set for early stopping\n",
    "        early_stopping_rounds=50  # Stop if the eval metric doesn't improve for 50 rounds\n",
    "    )\n",
    "    \n",
    "    # Predict on the validation set using the best iteration\n",
    "    y_valid_pred = model.predict(dvalid)\n",
    "    \n",
    "    # Calculate the log loss on the validation set\n",
    "    loss = log_loss(y_valid, y_valid_pred)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create an optuna study and specifcy the the direction of the optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(f'Accuracy: {trial.value}')\n",
    "print('Best hyperparameters: {}'.format(trial.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Fastball Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure trial is the best trial from the study\n",
    "best_trial = study.best_trial\n",
    "\n",
    "# Set parameters\n",
    "final_parameters = {\n",
    "    'max_depth': best_trial.params['max_depth'],\n",
    "    'eta': best_trial.params['eta'],\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'subsample': best_trial.params['subsample'],\n",
    "    'colsample_bytree': best_trial.params['colsample_bytree'],\n",
    "    'scale_pos_weight' : class_ratio\n",
    "}\n",
    "\n",
    "# Prepare the data\n",
    "dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "# Optionally, prepare a validation set\n",
    "#dval = xgb.DMatrix(x_val, label=y_val)\n",
    "\n",
    "# Train the final model\n",
    "# Optionally, include early stopping\n",
    "# final_model = xgb.train(final_parameters, dtrain, early_stopping_rounds=10, evals=[(dval, 'eval')])\n",
    "final_model = xgb.train(final_parameters, dtrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred = final_model.predict(dtest)\n",
    "y_pred_binary = [1 if p >= 0.5 else 0 for p in y_pred]\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate Log Loss\n",
    "logloss = log_loss(y_test, y_pred)\n",
    "print(f\"Log Loss: {logloss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = final_model.get_score(importance_type='gain')\n",
    "# Visualizing the feature importances\n",
    "xgb.plot_importance(importance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Data Set Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = xgb.DMatrix(testing_data, label=testing_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = final_model.predict(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_identifiers['xfoul'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_identifiers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_identifiers.to_csv(\"Y:/departments/research_and_development/baseball_operations/clayton_goodiez/csv/cs_2022&3_xcalledstrike.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offspeed Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "off = offspeed_data.drop(['description', 'events', 'game_pk', 'game_date', 'des', 'pitch_type_dict',\n",
    "                     'spin_rate_deprecated', 'break_angle_deprecated', 'break_length_deprecated', 'game_type', 'home_team',\n",
    "                     'away_team', 'type', 'bb_type', 'inning_topbot', 'hc_x', 'hc_y', 'tfs_deprecated', 'tfs_zulu_deprecated',\n",
    "                     'hit_distance_sc', 'launch_speed', 'launch_angle', 'estimated_ba_using_speedangle', 'estimated_woba_using_speedangle',\n",
    "                     'woba_value', 'woba_denom', 'babip_value', 'iso_value', 'launch_speed_angle', 'pitch_name', 'home_score', 'away_score',\n",
    "                     'bat_score', 'fld_score', 'post_away_score', 'post_home_score', 'post_bat_score', 'post_fld_score',\n",
    "                      'if_fielding_alignment', 'of_fielding_alignment', 'delta_home_win_exp', 'delta_run_exp',\n",
    "                      'player_name', 'hit_location', 'umpire', 'sv_id', 'spin_dir', \n",
    "                      'pitch_type', 'fielder_2.1', 'pitcher.1', 'fielder_3', 'fielder_4', 'fielder_5', 'fielder_6', 'fielder_7', 'fielder_8', \n",
    "                      'fielder_9', 'batter', 'pitcher', 'on_3b', 'on_2b', 'on_1b', 'game_year', 'outs_when_up', 'pitch_number',  \n",
    "                      'at_bat_number', 'inning', 'fielder_2', 'sz_top', 'sz_bot',\n",
    "                      #removed these columns because of collinearity\n",
    "                      'release_pos_z',  'effective_speed', 'release_pos_y', 'vy0', 'vx0', 'ax', 'release_extension', 'spin_axis', 'release_spin_rate',\n",
    "                      'pfx_z', 'ay', 'p_throws', 'az', 'avg_velo', 'avg_hmov', 'avg_vmov', 'pfx_x', 'Unnamed: 0', 'release_pos_x', 'balls', \n",
    "                        'total_movement', 'vmov_diff', 'hmov_diff', 'velo_diff', 'vz0', 'release_speed'\n",
    "                      #'release_spin_rate', 'p_throws', 'spin_axis', 'zone',\n",
    "                      #removed these columns because they are not useful\n",
    "                      ], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = off.drop(['is_strike'], axis=1).copy()\n",
    "y2 = off['is_strike'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(x2, y2, test_size=0.2, random_state=42) # split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain1 = xgb.DMatrix(x_train1, label=y_train1)\n",
    "dtest1 = xgb.DMatrix(x_test1, label=y_test1)\n",
    "\n",
    "params1 = {\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.3,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 1\n",
    "}\n",
    "\n",
    "# Initial Model\n",
    "model2 = xgb.train(params1, dtrain1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique labels in y_test1: {np.unique(y_test1)}\")\n",
    "print(f\"Unique labels in y_train1: {np.unique(y_train1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred1 = model2.predict(dtest1)\n",
    "y_pred_binary1 = [1 if p >= 0.5 else 0 for p in y_pred1]\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(y_test1, y_pred_binary1)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate Log Loss\n",
    "logloss = log_loss(y_test1, y_pred1)\n",
    "print(f\"Log Loss: {logloss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use an objective function to find the best parameters\n",
    "def objectives(trial):\n",
    "    dtrain1 = xgb.DMatrix(x_train1, label=y_train1)\n",
    "    dtest1 = xgb.DMatrix(x_test1, label=y_test1)\n",
    "    \n",
    "    param1 = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3),\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1),\n",
    "    }\n",
    "\n",
    "    model2 = xgb.train(param1, dtrain1)\n",
    "    y_pred1= model2.predict(dtest1)\n",
    "    y_pred_binary1 = [1 if p >= 0.5 else 0 for p in y_pred1]\n",
    "    accuracy = accuracy_score(y_test1, y_pred_binary1)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an optuna study and specifcy the the direction of the optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objectives, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(f'Accuracy: {trial.value}')\n",
    "print('Best hyperparameters: {}'.format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure trial is the best trial from the study\n",
    "best_trial1 = study.best_trial\n",
    "\n",
    "# Set parameters\n",
    "final_parameters1 = {\n",
    "    'max_depth': best_trial.params['max_depth'],\n",
    "    'eta': best_trial.params['eta'],\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'subsample': best_trial.params['subsample'],\n",
    "    'colsample_bytree': best_trial.params['colsample_bytree'],\n",
    "}\n",
    "\n",
    "# Prepare the data\n",
    "dtrain = xgb.DMatrix(x_train1, label=y_train1)\n",
    "# Optionally, prepare a validation set\n",
    "#dval = xgb.DMatrix(x_val, label=y_val)\n",
    "\n",
    "# Train the final model\n",
    "# Optionally, include early stopping\n",
    "# final_model = xgb.train(final_parameters, dtrain, early_stopping_rounds=10, evals=[(dval, 'eval')])\n",
    "final_model1 = xgb.train(final_parameters1, dtrain1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred1 = final_model1.predict(dtest1)\n",
    "y_pred_binary1 = [1 if p >= 0.5 else 0 for p in y_pred1]\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(y_test1, y_pred_binary1)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate Log Loss\n",
    "logloss = log_loss(y_test1, y_pred1)\n",
    "print(f\"Log Loss: {logloss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance1 = final_model1.get_score(importance_type='gain')\n",
    "# Visualizing the feature importances\n",
    "xgb.plot_importance(importance1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict 2023 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heaters_2023 = df[df['pitch_type_dict'] == 1].copy()\n",
    "offspeed_2023 = df[df['pitch_type_dict'] == 2].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the features you used for training the models\n",
    "features = ['zone', 'stand', 'strikes', 'plate_x', 'plate_z',  'likely_strike']  # Replace these with your actual features\n",
    "\n",
    "# Convert DataFrame to DMatrix\n",
    "dmatrix_heaters_2023 = xgb.DMatrix(heaters_2023[features])\n",
    "dmatrix_offspeed_2023 = xgb.DMatrix(offspeed_2023[features])\n",
    "\n",
    "# Make predictions\n",
    "heaters_2023['prediction'] = final_model.predict(dmatrix_heaters_2023)\n",
    "offspeed_2023['prediction'] = final_model1.predict(dmatrix_offspeed_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete = pd.concat([heaters_2023, offspeed_2023], axis=0)\n",
    "complete['probability_added'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the conditions and assignment for 'probability_added'\n",
    "condition1 = (complete['likely_strike'] == 0) & (complete['is_strike'] == 1)\n",
    "complete.loc[condition1, 'probability_added'] = 1 - complete.loc[condition1, 'prediction']\n",
    "\n",
    "condition2 = (complete['likely_strike'] == 1) & (complete['is_strike'] == 0)\n",
    "complete.loc[condition2, 'probability_added'] = -0 + complete.loc[condition2, 'prediction']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(condition2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = pd.read_csv('players_query.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed = complete.merge(players, left_on='fielder_2', right_on='mlb_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strike_probs = completed.loc[completed['probability_added'] != 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strike_probs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catcher Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display options to show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "result = strike_probs.groupby('mlb_name').agg({'probability_added': 'sum'}).sort_values(by='probability_added', ascending=False)\n",
    "print(result)\n",
    "\n",
    "# Reset the display options to default\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strike_probs['framing_runs'] = (strike_probs['deltaRE'] * -1) * strike_probs['probability_added'].abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Situational Framing Runs Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display options to show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "result = strike_probs.groupby('mlb_name').agg({'framing_runs': 'sum'}).sort_values(by='framing_runs', ascending=False)\n",
    "print(result)\n",
    "\n",
    "# Reset the display options to default\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strike_probs.drop(['Unnamed: 0', 'Unnamed: 1'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strike_probs.to_csv('strike_probs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get out meaningless strikes that have nothing to do with framing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likely = strike_probs.query('(prediction < .85)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display options to show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "result = likely.groupby('mlb_name').agg({'framing_runs': 'sum'}).sort_values(by='framing_runs', ascending=False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  I think that this is the best leaderboard. Filters out the gimme pitches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
